{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29089c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "file = \"reddit_dataframe.pkl\"\n",
    "df1 = pd.read_pickle(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f19f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs=df1.copy()\n",
    "dfs=dfs.iloc[0:100,:] #資料太大很耗時, 這裡我用小部分來做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46013043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "RE_SUSPICIOUS = re.compile(r'[&#<>{}\\[\\]\\\\]')\n",
    "\n",
    "def impurity(text, min_len=10):\n",
    "    \"\"\"returns the share of suspicious characters in a text\"\"\"\n",
    "    if text == None or len(text) < min_len:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(RE_SUSPICIOUS.findall(text))/len(text)\n",
    "    \n",
    "dfs['impurity'] = dfs['text'].apply(impurity, min_len=10)    \n",
    "\n",
    "\n",
    "# get the top 3 records\n",
    "dfs[['text', 'impurity']].sort_values(by='impurity', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2c2916",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['text'][68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "###Part II: Removing Nose with Regular Expressions###\n",
    "#####################################################\n",
    "#remark: html.unescape\n",
    "import html\n",
    "p = '&lt;abc&gt;' #&lt; and &gt; are special simbles in html\n",
    "#not showing in text example\n",
    "txt= html.unescape(p)\n",
    "print (txt)\n",
    "\n",
    "import html\n",
    "\n",
    "def clean(text):\n",
    "    # convert html escapes like &amp; to characters.\n",
    "    text = html.unescape(text) #in this example, this part does nothing\n",
    "    # tags like <tab>\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    # markdown URLs like [Some text](https://....)\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', ' ', text)\n",
    "    # text or code in brackets like [0]\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    # standalone sequences of specials, matches &# but not #cool\n",
    "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "    # standalone sequences of hyphens like --- or ==\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
    "    # sequences of white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "dfs['clean_text'] = dfs['text'].apply(clean)\n",
    "dfs['impurity']   = dfs['clean_text'].apply(impurity, min_len=20)\n",
    "\n",
    "dfs[['clean_text', 'impurity']].sort_values(by='impurity', ascending=False) \\\n",
    "                              .head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf02bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "###Part III: Character Normalization with textacy###\n",
    "####################################################\n",
    "import textacy.preprocessing as tprep\n",
    "#you need to install textacy\n",
    "def normalize(text):\n",
    "    text = tprep.normalize.hyphenated_words(text)\n",
    "    text = tprep.normalize.quotation_marks(text)\n",
    "    text = tprep.normalize.unicode(text)\n",
    "    text = tprep.remove.accents(text)\n",
    "    return text\n",
    "\n",
    "dfs['clean_text'] = dfs['clean_text'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07369669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "###Part IV: Character Masking with textacy###\n",
    "############################################# \n",
    "\n",
    "from textacy.preprocessing import replace\n",
    "dfs['clean_text'] = dfs['clean_text'].apply(replace.urls)\n",
    "\n",
    "##最後整理\n",
    "dfs.rename(columns={'text': 'raw_text', 'clean_text': 'text'}, inplace=True)\n",
    "dfs.drop(columns=['impurity'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a932d033",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d498b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "###Liguistic Processing###\n",
    "##########################\n",
    "\n",
    "#All steps in one by using spacy\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "dfs['doc']=dfs['text'].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862e0411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "\n",
    "def extract_lemmas(doc, **kwargs):\n",
    "    return [t.lemma_ for t in textacy.extract.basics.words(doc, **kwargs)]\n",
    "\n",
    "\n",
    "dfs['lemmas'] = dfs['doc'].apply(extract_lemmas, include_pos=['ADJ', 'NOUN'])\n",
    "dfs['lemmas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd3d7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "#Freq Charts#\n",
    "#############\n",
    "\n",
    "from collections import Counter\n",
    "counter = Counter()#use a empty string first\n",
    "dfs['lemmas'].map(counter.update)\n",
    "\n",
    "print(counter.most_common(5))\n",
    "# transform counter into data frame\n",
    "min_freq=2\n",
    "#transform dict into dataframe\n",
    "freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "freq_df = freq_df.query('freq >= @min_freq')\n",
    "freq_df.index.name = 'token'\n",
    "freq_df = freq_df.sort_values('freq', ascending=False)\n",
    "freq_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efa582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = freq_df.head(15).plot(kind='barh', width=0.95, figsize=(8,3))\n",
    "ax.invert_yaxis()\n",
    "ax.set(xlabel='Frequency', ylabel='Token', title='Top Words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f7309",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Creating Word Clouds\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud ###\n",
    "from collections import Counter ###\n",
    "\n",
    "wordcloud = WordCloud(font_path=\"SimHei.ttf\", background_color=\"white\")\n",
    "wordcloud.generate_from_frequencies(freq_df['freq'])\n",
    "#plt.figure(figsize=(20,10)) \n",
    "plt.imshow(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642fece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['lemmas'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f3b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs1=dfs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70e8b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_string(org_list, seperator=' '):\n",
    "    return seperator.join(org_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f5c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs1['C_lemmas']=dfs1['lemmas'].apply(list_to_string)\n",
    "dfs1['C_lemmas'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9feece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(decode_error='ignore', min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5d0465",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt01 = cv.fit_transform(dfs1['C_lemmas'])\n",
    "print(cv.get_feature_names())\n",
    "fn=cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe57a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dtmatrix=pd.DataFrame(dt01.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fba72a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c17e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(dt01[30], dt01[63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baaea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = pd.DataFrame(cosine_similarity(dt01, dt01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d372c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs1.iloc[98,:].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d0a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs1.iloc[12,:].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34267bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dt = tfidf.fit_transform(dt01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf94e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfmatrix = pd.DataFrame(tfidf_dt.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a23001",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm1 =pd.DataFrame(cosine_similarity(tfidf_dt, tfidf_dt))\n",
    "#sm1 =pd.DataFrame(cosine_similarity(tfidf_dt.T, tfidf_dt.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56478329",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9befe090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud ###\n",
    "from collections import Counter ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b1aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfsum=tfidfmatrix.T.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9bbafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(font_path=\"SimHei.ttf\", background_color=\"white\")\n",
    "wordcloud.generate_from_frequencies(tfidfsum)\n",
    "#plt.figure(figsize=(20,10)) \n",
    "plt.imshow(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b46ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb70a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "for i in range(1, 5):\n",
    "    km = KMeans(\n",
    "        n_clusters=i, init='random',\n",
    "        n_init=10, max_iter=300,\n",
    "        tol=1e-04, random_state=0\n",
    "    )\n",
    "    km.fit(preprocessing.normalize(tfidf_dt))\n",
    "    distortions.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60083829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(range(1, 5), distortions, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d8ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(\n",
    "    n_clusters=5, init='random',\n",
    "    n_init=10, max_iter=300, \n",
    "    tol=1e-04, random_state=0\n",
    ")\n",
    "y_km = km.fit_predict(preprocessing.normalize(tfidf_dt))\n",
    "\n",
    "g0 = dfs1['text'][y_km==0]\n",
    "g0.head()\n",
    "g1 = dfs1['text'][y_km==1]\n",
    "g1.head()\n",
    "g2 = dfs1['text'][y_km==2]\n",
    "g2.head()\n",
    "g3 = dfs1['text'][y_km==3]\n",
    "g3.head()\n",
    "g4 = dfs1['text'][y_km==4]\n",
    "g4.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
